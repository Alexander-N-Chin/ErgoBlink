{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 0. Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp # Import mediapipe\n",
    "import cv2 # Import opencv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils # Drawing helpers\n",
    "mp_holistic = mp.solutions.holistic # Mediapipe Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Author: Asadullah Dal\n",
    "Youtube Channel: https://www.youtube.com/c/aiphile\n",
    "'''\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# colors\n",
    "# values =(blue, green, red) opencv accepts BGR values not RGB\n",
    "BLACK = (0,0,0)\n",
    "WHITE = (255,255,255)\n",
    "BLUE = (255,0,0)\n",
    "RED = (0,0,255)\n",
    "CYAN = (255,255,0)\n",
    "YELLOW =(0,255,255)\n",
    "MAGENTA = (255,0,255)\n",
    "GRAY = (128,128,128)\n",
    "GREEN = (0,255,0)\n",
    "PURPLE = (128,0,128)\n",
    "ORANGE = (0,165,255)\n",
    "PINK = (147,20,255)\n",
    "points_list =[(200, 300), (150, 150), (400, 200)]\n",
    "def drawColor(img, colors):\n",
    "    x, y = 0,10\n",
    "    w, h = 20, 30\n",
    "\n",
    "    for color in colors:\n",
    "        x += w+5\n",
    "        # y += 10\n",
    "        cv.rectangle(img, (x-6, y-5 ), (x+w+5, y+h+5), (10, 50, 10), -1)\n",
    "        cv.rectangle(img, (x, y ), (x+w, y+h), color, -1)\n",
    "\n",
    "def colorBackgroundText(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0), bgColor=(0,0,0), pad_x=3, pad_y=3):\n",
    "    \"\"\"\n",
    "    Draws text with background, with  control transparency\n",
    "    @param img:(mat) which you want to draw text\n",
    "    @param text: (string) text you want draw\n",
    "    @param font: fonts face, like FONT_HERSHEY_COMPLEX, FONT_HERSHEY_PLAIN etc.\n",
    "    @param fontScale: (double) the size of text, how big it should be.\n",
    "    @param textPos: tuple(x,y) position where you want to draw text\n",
    "    @param textThickness:(int) fonts weight, how bold it should be\n",
    "    @param textPos: tuple(x,y) position where you want to draw text\n",
    "    @param textThickness:(int) fonts weight, how bold it should be.\n",
    "    @param textColor: tuple(BGR), values -->0 to 255 each\n",
    "    @param bgColor: tuple(BGR), values -->0 to 255 each\n",
    "    @param pad_x: int(pixels)  padding of in x direction\n",
    "    @param pad_y: int(pixels) 1 to 1.0 (), controls transparency of  text background\n",
    "    @return: img(mat) with draw with background\n",
    "    \"\"\"\n",
    "    (t_w, t_h), _= cv.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
    "    x, y = textPos\n",
    "    cv.rectangle(img, (x-pad_x, y+ pad_y), (x+t_w+pad_x, y-t_h-pad_y), bgColor,-1) # draw rectangle\n",
    "    cv.putText(img,text, textPos,font, fontScale, textColor,textThickness ) # draw in text\n",
    "\n",
    "    return img\n",
    "\n",
    "def textWithBackground(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0), bgColor=(0,0,0), pad_x=3, pad_y=3, bgOpacity=0.5):\n",
    "    \"\"\"\n",
    "    Draws text with background, with  control transparency\n",
    "    @param img:(mat) which you want to draw text\n",
    "    @param text: (string) text you want draw\n",
    "    @param font: fonts face, like FONT_HERSHEY_COMPLEX, FONT_HERSHEY_PLAIN etc.\n",
    "    @param fontScale: (double) the size of text, how big it should be.\n",
    "    @param textPos: tuple(x,y) position where you want to draw text\n",
    "    @param textThickness:(int) fonts weight, how bold it should be\n",
    "    @param textPos: tuple(x,y) position where you want to draw text\n",
    "    @param textThickness:(int) fonts weight, how bold it should be.\n",
    "    @param textColor: tuple(BGR), values -->0 to 255 each\n",
    "    @param bgColor: tuple(BGR), values -->0 to 255 each\n",
    "    @param pad_x: int(pixels)  padding of in x direction\n",
    "    @param pad_y: int(pixels) 1 to 1.0 (), controls transparency of  text background\n",
    "    @return: img(mat) with draw with background\n",
    "    \"\"\"\n",
    "    (t_w, t_h), _= cv.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
    "    x, y = textPos\n",
    "    overlay = img.copy() # coping the image\n",
    "    cv.rectangle(overlay, (x-pad_x, y+ pad_y), (x+t_w+pad_x, y-t_h-pad_y), bgColor,-1) # draw rectangle\n",
    "    new_img = cv.addWeighted(overlay, bgOpacity, img, 1 - bgOpacity, 0) # overlaying the rectangle on the image.\n",
    "    cv.putText(new_img,text, textPos,font, fontScale, textColor,textThickness ) # draw in text\n",
    "    img = new_img\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def textBlurBackground(img, text, font, fontScale, textPos, textThickness=1,textColor=(0,255,0),kneral=(33,33) , pad_x=3, pad_y=3):\n",
    "    \"\"\"\n",
    "    Draw text with background blured,  control the blur value, with kernal(odd, odd)\n",
    "    @param img:(mat) which you want to draw text\n",
    "    @param text: (string) text you want draw\n",
    "    @param font: fonts face, like FONT_HERSHEY_COMPLEX, FONT_HERSHEY_PLAIN etc.\n",
    "    @param fontScale: (double) the size of text, how big it should be.\n",
    "    @param textPos: tuple(x,y) position where you want to draw text\n",
    "    @param textThickness:(int) fonts weight, how bold it should be.\n",
    "    @param textColor: tuple(BGR), values -->0 to 255 each\n",
    "    @param kneral: tuple(3,3) int as odd number:  higher the value, more blurry background would be\n",
    "    @param pad_x: int(pixels)  padding of in x direction\n",
    "    @param pad_y: int(pixels)  padding of in y direction\n",
    "    @return: img mat, with text drawn, with background blured\n",
    "\n",
    "    call the function:\n",
    "     img =textBlurBackground(img, 'Blured Background Text', cv2.FONT_HERSHEY_COMPLEX, 0.9, (20, 60),2, (0,255, 0), (49,49), 13, 13 )\n",
    "    \"\"\"\n",
    "\n",
    "    (t_w, t_h), _= cv.getTextSize(text, font, fontScale, textThickness) # getting the text size\n",
    "    x, y = textPos\n",
    "    blur_roi = img[y-pad_y-t_h: y+pad_y, x-pad_x:x+t_w+pad_x] # croping Text Background\n",
    "    img[y-pad_y-t_h: y+pad_y, x-pad_x:x+t_w+pad_x]=cv.blur(blur_roi, kneral)  # merging the blured background to img\n",
    "    cv.putText(img,text, textPos,font, fontScale, textColor,textThickness )\n",
    "    # cv.imshow('blur roi', blur_roi)\n",
    "    # cv.imshow('blured', img)\n",
    "\n",
    "    return img\n",
    "\n",
    "def fillPolyTrans(img, points, color, opacity):\n",
    "    \"\"\"\n",
    "    @param img: (mat) input image, where shape is drawn.\n",
    "    @param points: list [tuples(int, int) these are the points custom shape,FillPoly\n",
    "    @param color: (tuples (int, int, int)\n",
    "    @param opacity:  it is transparency of image.\n",
    "    @return: img(mat) image with rectangle draw.\n",
    "    \"\"\"\n",
    "    list_to_np_array = np.array(points, dtype=np.int32)\n",
    "    overlay = img.copy()  # coping the image\n",
    "    cv.fillPoly(overlay,[list_to_np_array], color )\n",
    "    new_img = cv.addWeighted(overlay, opacity, img, 1 - opacity, 0)\n",
    "    # print(points_list)\n",
    "    img = new_img\n",
    "    cv.polylines(img, [list_to_np_array], True, color,1, cv.LINE_AA)\n",
    "    return img\n",
    "\n",
    "# def pollyLines(img, points, color):\n",
    "#     list_to_np_array = np.array(points, dtype=np.int32)\n",
    "#     cv.polylines(img, [list_to_np_array], True, color,1, cv.LINE_AA)\n",
    "#     return img\n",
    "\n",
    "def rectTrans(img, pt1, pt2, color, thickness, opacity):\n",
    "    \"\"\"\n",
    "    @param img: (mat) input image, where shape is drawn.\n",
    "    @param pt1: tuple(int,int) it specifies the starting point(x,y) os rectangle\n",
    "    @param pt2: tuple(int,int)  it nothing but width and height of rectangle\n",
    "    @param color: (tuples (int, int, int), it tuples of BGR values\n",
    "    @param thickness: it thickness of board line rectangle, if (-1) passed then rectangle will be fulled with color.\n",
    "    @param opacity:  it is transparency of image.\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    overlay = img.copy()\n",
    "    cv.rectangle(overlay, pt1, pt2, color, thickness)\n",
    "    new_img = cv.addWeighted(overlay, opacity, img, 1 - opacity, 0) # overlaying the rectangle on the image.\n",
    "    img = new_img\n",
    "\n",
    "    return img\n",
    "\n",
    "def main():\n",
    "    cap = cv.VideoCapture('Girl.mp4')\n",
    "    counter =0\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        # img = np.zeros((1000,1000, 3), dtype=np.uint8)\n",
    "        img=rectTrans(img, pt1=(30, 320), pt2=(160, 260), color=(0,255,255),thickness=-1, opacity=0.6)\n",
    "        img =fillPolyTrans(img=img, points=points_list, color=(0,255,0), opacity=.5)\n",
    "        drawColor(img, [BLACK,WHITE ,BLUE,RED,CYAN,YELLOW,MAGENTA,GRAY ,GREEN,PURPLE,ORANGE,PINK])\n",
    "        textBlurBackground(img, 'Blured Background Text', cv.FONT_HERSHEY_COMPLEX, 0.8, (60, 140),2, YELLOW, (71,71), 13, 13)\n",
    "        img=textWithBackground(img, 'Colored Background Texts', cv.FONT_HERSHEY_SIMPLEX, 0.8, (60,80), textThickness=2, bgColor=GREEN, textColor=BLACK, bgOpacity=0.7, pad_x=6, pad_y=6)\n",
    "        imgGray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        # cv.imwrite('color_image.png', img)\n",
    "        counter +=1\n",
    "        cv.imshow('img', img)\n",
    "        cv.imwrite(f'image/image_{counter}.png', img)\n",
    "        if cv.waitKey(1) ==ord('q'):\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import utils, math\n",
    "import numpy as np\n",
    "# variables\n",
    "frame_counter =0\n",
    "CEF_COUNTER =0\n",
    "TOTAL_BLINKS =0\n",
    "# constants\n",
    "CLOSED_EYES_FRAME =3\n",
    "FONTS =cv.FONT_HERSHEY_COMPLEX\n",
    "\n",
    "# face bounder indices\n",
    "FACE_OVAL=[ 10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103,67, 109]\n",
    "\n",
    "# lips indices for Landmarks\n",
    "LIPS=[ 61, 146, 91, 181, 84, 17, 314, 405, 321, 375,291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95,185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78 ]\n",
    "LOWER_LIPS =[61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]\n",
    "UPPER_LIPS=[ 185, 40, 39, 37,0 ,267 ,269 ,270 ,409, 415, 310, 311, 312, 13, 82, 81, 42, 183, 78]\n",
    "# Left eyes indices\n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "LEFT_EYEBROW =[ 336, 296, 334, 293, 300, 276, 283, 282, 295, 285 ]\n",
    "\n",
    "# right eyes indices\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]\n",
    "RIGHT_EYEBROW=[ 70, 63, 105, 66, 107, 55, 65, 52, 53, 46 ]\n",
    "\n",
    "map_face_mesh = mp.solutions.face_mesh\n",
    "# camera object\n",
    "camera = cv.VideoCapture(1)\n",
    "# landmark detection function\n",
    "def landmarksDetection(img, results, draw=False):\n",
    "    img_height, img_width= img.shape[:2]\n",
    "    # list[(x,y), (x,y)....]\n",
    "    mesh_coord = [(int(point.x * img_width), int(point.y * img_height)) for point in results.multi_face_landmarks[0].landmark]\n",
    "    if draw :\n",
    "        [cv.circle(img, p, 2, (0,255,0), -1) for p in mesh_coord]\n",
    "\n",
    "    # returning the list of tuples for each landmarks\n",
    "    return mesh_coord\n",
    "\n",
    "# Euclaidean distance\n",
    "def euclaideanDistance(point, point1):\n",
    "    x, y = point\n",
    "    x1, y1 = point1\n",
    "    distance = math.sqrt((x1 - x)**2 + (y1 - y)**2)\n",
    "    return distance\n",
    "\n",
    "# Blinking Ratio\n",
    "def blinkRatio(img, landmarks, right_indices, left_indices):\n",
    "    # Right eyes\n",
    "    # horizontal line\n",
    "    rh_right = landmarks[right_indices[0]]\n",
    "    rh_left = landmarks[right_indices[8]]\n",
    "    # vertical line\n",
    "    rv_top = landmarks[right_indices[12]]\n",
    "    rv_bottom = landmarks[right_indices[4]]\n",
    "    # draw lines on right eyes\n",
    "    # cv.line(img, rh_right, rh_left, utils.GREEN, 2)\n",
    "    # cv.line(img, rv_top, rv_bottom, utils.WHITE, 2)\n",
    "\n",
    "    # LEFT_EYE\n",
    "    # horizontal line\n",
    "    lh_right = landmarks[left_indices[0]]\n",
    "    lh_left = landmarks[left_indices[8]]\n",
    "\n",
    "    # vertical line\n",
    "    lv_top = landmarks[left_indices[12]]\n",
    "    lv_bottom = landmarks[left_indices[4]]\n",
    "\n",
    "    rhDistance = euclaideanDistance(rh_right, rh_left)\n",
    "    rvDistance = euclaideanDistance(rv_top, rv_bottom)\n",
    "\n",
    "    lvDistance = euclaideanDistance(lv_top, lv_bottom)\n",
    "    lhDistance = euclaideanDistance(lh_right, lh_left)\n",
    "\n",
    "    reRatio = rhDistance/rvDistance\n",
    "    leRatio = lhDistance/lvDistance\n",
    "\n",
    "    ratio = (reRatio+leRatio)/2\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "with map_face_mesh.FaceMesh(min_detection_confidence =0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "\n",
    "    # starting time here\n",
    "    start_time = time.time()\n",
    "    # starting Video loop here.\n",
    "    while True:\n",
    "        frame_counter +=1 # frame counter\n",
    "        ret, frame = camera.read() # getting frame from camera\n",
    "        if not ret:\n",
    "            break # no more frames break\n",
    "        #  resizing frame\n",
    "\n",
    "        frame = cv.resize(frame, None, fx=1.5, fy=1.5, interpolation=cv.INTER_CUBIC)\n",
    "        frame_height, frame_width= frame.shape[:2]\n",
    "        rgb_frame = cv.cvtColor(frame, cv.COLOR_RGB2BGR)\n",
    "        results  = face_mesh.process(rgb_frame)\n",
    "        if results.multi_face_landmarks:\n",
    "            mesh_coords = landmarksDetection(frame, results, False)\n",
    "            ratio = blinkRatio(frame, mesh_coords, RIGHT_EYE, LEFT_EYE)\n",
    "            # cv.putText(frame, f'ratio {ratio}', (100, 100), FONTS, 1.0, utils.GREEN, 2)\n",
    "            utils.colorBackgroundText(frame,  f'Ratio : {round(ratio,2)}', FONTS, 0.7, (30,100),2, utils.PINK, utils.YELLOW)\n",
    "\n",
    "            if ratio >5.5:\n",
    "                CEF_COUNTER +=1\n",
    "                # cv.putText(frame, 'Blink', (200, 50), FONTS, 1.3, utils.PINK, 2)\n",
    "                utils.colorBackgroundText(frame,  f'Blink', FONTS, 1.7, (int(frame_height/2), 100), 2, utils.YELLOW, pad_x=6, pad_y=6, )\n",
    "\n",
    "            else:\n",
    "                if CEF_COUNTER>CLOSED_EYES_FRAME:\n",
    "                    TOTAL_BLINKS +=1\n",
    "                    CEF_COUNTER =0\n",
    "            # cv.putText(frame, f'Total Blinks: {TOTAL_BLINKS}', (100, 150), FONTS, 0.6, utils.GREEN, 2)\n",
    "            utils.colorBackgroundText(frame,  f'Total Blinks: {TOTAL_BLINKS}', FONTS, 0.7, (30,150),2)\n",
    "\n",
    "            cv.polylines(frame,  [np.array([mesh_coords[p] for p in LEFT_EYE ], dtype=np.int32)], True, utils.GREEN, 1, cv.LINE_AA)\n",
    "            cv.polylines(frame,  [np.array([mesh_coords[p] for p in RIGHT_EYE ], dtype=np.int32)], True, utils.GREEN, 1, cv.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "        # calculating  frame per seconds FPS\n",
    "        end_time = time.time()-start_time\n",
    "        fps = frame_counter/end_time\n",
    "\n",
    "        frame =utils.textWithBackground(frame,f'FPS: {round(fps,1)}',FONTS, 1.0, (30, 50), bgOpacity=0.9, textThickness=2)\n",
    "        # writing image for thumbnail drawing shape\n",
    "        # cv.imwrite(f'img/frame_{frame_counter}.png', frame)\n",
    "        cv.imshow('frame', frame)\n",
    "        key = cv.waitKey(2)\n",
    "        if key==ord('q') or key ==ord('Q'):\n",
    "            break\n",
    "    cv.destroyAllWindows()\n",
    "    camera.release()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Make Some Detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Left eyes indices\n",
    "LEFT_EYE =[ 362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385,384, 398 ]\n",
    "LEFT_EYEBROW =[ 336, 296, 334, 293, 300, 276, 283, 282, 295, 285 ]\n",
    "\n",
    "# right eyes indices\n",
    "RIGHT_EYE=[ 33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161 , 246 ]\n",
    "RIGHT_EYEBROW=[ 70, 63, 105, 66, 107, 55, 65, 52, 53, 46 ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def euclaideanDistance(point, point1):\n",
    "    x, y = point\n",
    "    x1, y1 = point1\n",
    "    distance = math.sqrt((x1 - x)**2 + (y1 - y)**2)\n",
    "    return distance\n",
    "\n",
    "# Blinking Ratio\n",
    "def blinkRatio(img, landmarks, right_indices, left_indices):\n",
    "    # Right eyes\n",
    "    # horizontal linez\n",
    "    rh_right = landmarks[right_indices[0]].values()\n",
    "    rh_left = landmarks[right_indices[8]].values()\n",
    "    print(rh_left)\n",
    "    # vertical line\n",
    "    rv_top = landmarks[right_indices[12]].values()\n",
    "    rv_bottom = landmarks[right_indices[4]].values()\n",
    "    # draw lines on right eyes\n",
    "    # cv.line(img, rh_right, rh_left, utils.GREEN, 2)\n",
    "    # cv.line(img, rv_top, rv_bottom, utils.WHITE, 2)\n",
    "\n",
    "    # LEFT_EYE\n",
    "    # horizontal line\n",
    "    lh_right = landmarks[left_indices[0]].values()\n",
    "    lh_left = landmarks[left_indices[8]].values()\n",
    "\n",
    "    # vertical line\n",
    "    lv_top = landmarks[left_indices[12]].values()\n",
    "    lv_bottom = landmarks[left_indices[4]].values()\n",
    "\n",
    "    rhDistance = euclaideanDistance(rh_right, rh_left)\n",
    "    rvDistance = euclaideanDistance(rv_top, rv_bottom)\n",
    "\n",
    "    lvDistance = euclaideanDistance(lv_top, lv_bottom)\n",
    "    lhDistance = euclaideanDistance(lh_right, lh_left)\n",
    "\n",
    "    reRatio = rhDistance/rvDistance\n",
    "    leRatio = lhDistance/lvDistance\n",
    "\n",
    "    ratio = (reRatio+leRatio)/2\n",
    "    return ratio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "CEF_COUNTER = 0\n",
    "TOTAL_BLINKS = 0\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "\n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "\n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "\n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        try:\n",
    "\n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # print(results.face_landmarks.landmark)\n",
    "        ratio = blinkRatio(image, results.face_landmarks.landmark, RIGHT_EYE, LEFT_EYE)\n",
    "        if ratio >5.5:\n",
    "            CEF_COUNTER +=1\n",
    "            # cv.putText(frame, 'Blink', (200, 50), FONTS, 1.3, utils.PINK, 2)\n",
    "            utils.colorBackgroundText(frame,  f'Blink', FONTS, 1.7, (int(frame_height/2), 100), 2, utils.YELLOW, pad_x=6, pad_y=6, )\n",
    "\n",
    "        else:\n",
    "            if CEF_COUNTER>CLOSED_EYES_FRAME:\n",
    "                TOTAL_BLINKS +=1\n",
    "                print(TOTAL_BLINKS)\n",
    "                CEF_COUNTER =0\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results.face_landmarks.landmark[0].visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Capture Landmarks & Export to CSV\n",
    "<!--<img src=\"https://i.imgur.com/8bForKY.png\">-->\n",
    "<!--<img src=\"https://i.imgur.com/AzKNp7A.png\">-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "468"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_coords = len(results.face_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "landmarks = ['class']\n",
    "for val in range(1, num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['class',\n 'x1',\n 'y1',\n 'z1',\n 'v1',\n 'x2',\n 'y2',\n 'z2',\n 'v2',\n 'x3',\n 'y3',\n 'z3',\n 'v3',\n 'x4',\n 'y4',\n 'z4',\n 'v4',\n 'x5',\n 'y5',\n 'z5',\n 'v5',\n 'x6',\n 'y6',\n 'z6',\n 'v6',\n 'x7',\n 'y7',\n 'z7',\n 'v7',\n 'x8',\n 'y8',\n 'z8',\n 'v8',\n 'x9',\n 'y9',\n 'z9',\n 'v9',\n 'x10',\n 'y10',\n 'z10',\n 'v10',\n 'x11',\n 'y11',\n 'z11',\n 'v11',\n 'x12',\n 'y12',\n 'z12',\n 'v12',\n 'x13',\n 'y13',\n 'z13',\n 'v13',\n 'x14',\n 'y14',\n 'z14',\n 'v14',\n 'x15',\n 'y15',\n 'z15',\n 'v15',\n 'x16',\n 'y16',\n 'z16',\n 'v16',\n 'x17',\n 'y17',\n 'z17',\n 'v17',\n 'x18',\n 'y18',\n 'z18',\n 'v18',\n 'x19',\n 'y19',\n 'z19',\n 'v19',\n 'x20',\n 'y20',\n 'z20',\n 'v20',\n 'x21',\n 'y21',\n 'z21',\n 'v21',\n 'x22',\n 'y22',\n 'z22',\n 'v22',\n 'x23',\n 'y23',\n 'z23',\n 'v23',\n 'x24',\n 'y24',\n 'z24',\n 'v24',\n 'x25',\n 'y25',\n 'z25',\n 'v25',\n 'x26',\n 'y26',\n 'z26',\n 'v26',\n 'x27',\n 'y27',\n 'z27',\n 'v27',\n 'x28',\n 'y28',\n 'z28',\n 'v28',\n 'x29',\n 'y29',\n 'z29',\n 'v29',\n 'x30',\n 'y30',\n 'z30',\n 'v30',\n 'x31',\n 'y31',\n 'z31',\n 'v31',\n 'x32',\n 'y32',\n 'z32',\n 'v32',\n 'x33',\n 'y33',\n 'z33',\n 'v33',\n 'x34',\n 'y34',\n 'z34',\n 'v34',\n 'x35',\n 'y35',\n 'z35',\n 'v35',\n 'x36',\n 'y36',\n 'z36',\n 'v36',\n 'x37',\n 'y37',\n 'z37',\n 'v37',\n 'x38',\n 'y38',\n 'z38',\n 'v38',\n 'x39',\n 'y39',\n 'z39',\n 'v39',\n 'x40',\n 'y40',\n 'z40',\n 'v40',\n 'x41',\n 'y41',\n 'z41',\n 'v41',\n 'x42',\n 'y42',\n 'z42',\n 'v42',\n 'x43',\n 'y43',\n 'z43',\n 'v43',\n 'x44',\n 'y44',\n 'z44',\n 'v44',\n 'x45',\n 'y45',\n 'z45',\n 'v45',\n 'x46',\n 'y46',\n 'z46',\n 'v46',\n 'x47',\n 'y47',\n 'z47',\n 'v47',\n 'x48',\n 'y48',\n 'z48',\n 'v48',\n 'x49',\n 'y49',\n 'z49',\n 'v49',\n 'x50',\n 'y50',\n 'z50',\n 'v50',\n 'x51',\n 'y51',\n 'z51',\n 'v51',\n 'x52',\n 'y52',\n 'z52',\n 'v52',\n 'x53',\n 'y53',\n 'z53',\n 'v53',\n 'x54',\n 'y54',\n 'z54',\n 'v54',\n 'x55',\n 'y55',\n 'z55',\n 'v55',\n 'x56',\n 'y56',\n 'z56',\n 'v56',\n 'x57',\n 'y57',\n 'z57',\n 'v57',\n 'x58',\n 'y58',\n 'z58',\n 'v58',\n 'x59',\n 'y59',\n 'z59',\n 'v59',\n 'x60',\n 'y60',\n 'z60',\n 'v60',\n 'x61',\n 'y61',\n 'z61',\n 'v61',\n 'x62',\n 'y62',\n 'z62',\n 'v62',\n 'x63',\n 'y63',\n 'z63',\n 'v63',\n 'x64',\n 'y64',\n 'z64',\n 'v64',\n 'x65',\n 'y65',\n 'z65',\n 'v65',\n 'x66',\n 'y66',\n 'z66',\n 'v66',\n 'x67',\n 'y67',\n 'z67',\n 'v67',\n 'x68',\n 'y68',\n 'z68',\n 'v68',\n 'x69',\n 'y69',\n 'z69',\n 'v69',\n 'x70',\n 'y70',\n 'z70',\n 'v70',\n 'x71',\n 'y71',\n 'z71',\n 'v71',\n 'x72',\n 'y72',\n 'z72',\n 'v72',\n 'x73',\n 'y73',\n 'z73',\n 'v73',\n 'x74',\n 'y74',\n 'z74',\n 'v74',\n 'x75',\n 'y75',\n 'z75',\n 'v75',\n 'x76',\n 'y76',\n 'z76',\n 'v76',\n 'x77',\n 'y77',\n 'z77',\n 'v77',\n 'x78',\n 'y78',\n 'z78',\n 'v78',\n 'x79',\n 'y79',\n 'z79',\n 'v79',\n 'x80',\n 'y80',\n 'z80',\n 'v80',\n 'x81',\n 'y81',\n 'z81',\n 'v81',\n 'x82',\n 'y82',\n 'z82',\n 'v82',\n 'x83',\n 'y83',\n 'z83',\n 'v83',\n 'x84',\n 'y84',\n 'z84',\n 'v84',\n 'x85',\n 'y85',\n 'z85',\n 'v85',\n 'x86',\n 'y86',\n 'z86',\n 'v86',\n 'x87',\n 'y87',\n 'z87',\n 'v87',\n 'x88',\n 'y88',\n 'z88',\n 'v88',\n 'x89',\n 'y89',\n 'z89',\n 'v89',\n 'x90',\n 'y90',\n 'z90',\n 'v90',\n 'x91',\n 'y91',\n 'z91',\n 'v91',\n 'x92',\n 'y92',\n 'z92',\n 'v92',\n 'x93',\n 'y93',\n 'z93',\n 'v93',\n 'x94',\n 'y94',\n 'z94',\n 'v94',\n 'x95',\n 'y95',\n 'z95',\n 'v95',\n 'x96',\n 'y96',\n 'z96',\n 'v96',\n 'x97',\n 'y97',\n 'z97',\n 'v97',\n 'x98',\n 'y98',\n 'z98',\n 'v98',\n 'x99',\n 'y99',\n 'z99',\n 'v99',\n 'x100',\n 'y100',\n 'z100',\n 'v100',\n 'x101',\n 'y101',\n 'z101',\n 'v101',\n 'x102',\n 'y102',\n 'z102',\n 'v102',\n 'x103',\n 'y103',\n 'z103',\n 'v103',\n 'x104',\n 'y104',\n 'z104',\n 'v104',\n 'x105',\n 'y105',\n 'z105',\n 'v105',\n 'x106',\n 'y106',\n 'z106',\n 'v106',\n 'x107',\n 'y107',\n 'z107',\n 'v107',\n 'x108',\n 'y108',\n 'z108',\n 'v108',\n 'x109',\n 'y109',\n 'z109',\n 'v109',\n 'x110',\n 'y110',\n 'z110',\n 'v110',\n 'x111',\n 'y111',\n 'z111',\n 'v111',\n 'x112',\n 'y112',\n 'z112',\n 'v112',\n 'x113',\n 'y113',\n 'z113',\n 'v113',\n 'x114',\n 'y114',\n 'z114',\n 'v114',\n 'x115',\n 'y115',\n 'z115',\n 'v115',\n 'x116',\n 'y116',\n 'z116',\n 'v116',\n 'x117',\n 'y117',\n 'z117',\n 'v117',\n 'x118',\n 'y118',\n 'z118',\n 'v118',\n 'x119',\n 'y119',\n 'z119',\n 'v119',\n 'x120',\n 'y120',\n 'z120',\n 'v120',\n 'x121',\n 'y121',\n 'z121',\n 'v121',\n 'x122',\n 'y122',\n 'z122',\n 'v122',\n 'x123',\n 'y123',\n 'z123',\n 'v123',\n 'x124',\n 'y124',\n 'z124',\n 'v124',\n 'x125',\n 'y125',\n 'z125',\n 'v125',\n 'x126',\n 'y126',\n 'z126',\n 'v126',\n 'x127',\n 'y127',\n 'z127',\n 'v127',\n 'x128',\n 'y128',\n 'z128',\n 'v128',\n 'x129',\n 'y129',\n 'z129',\n 'v129',\n 'x130',\n 'y130',\n 'z130',\n 'v130',\n 'x131',\n 'y131',\n 'z131',\n 'v131',\n 'x132',\n 'y132',\n 'z132',\n 'v132',\n 'x133',\n 'y133',\n 'z133',\n 'v133',\n 'x134',\n 'y134',\n 'z134',\n 'v134',\n 'x135',\n 'y135',\n 'z135',\n 'v135',\n 'x136',\n 'y136',\n 'z136',\n 'v136',\n 'x137',\n 'y137',\n 'z137',\n 'v137',\n 'x138',\n 'y138',\n 'z138',\n 'v138',\n 'x139',\n 'y139',\n 'z139',\n 'v139',\n 'x140',\n 'y140',\n 'z140',\n 'v140',\n 'x141',\n 'y141',\n 'z141',\n 'v141',\n 'x142',\n 'y142',\n 'z142',\n 'v142',\n 'x143',\n 'y143',\n 'z143',\n 'v143',\n 'x144',\n 'y144',\n 'z144',\n 'v144',\n 'x145',\n 'y145',\n 'z145',\n 'v145',\n 'x146',\n 'y146',\n 'z146',\n 'v146',\n 'x147',\n 'y147',\n 'z147',\n 'v147',\n 'x148',\n 'y148',\n 'z148',\n 'v148',\n 'x149',\n 'y149',\n 'z149',\n 'v149',\n 'x150',\n 'y150',\n 'z150',\n 'v150',\n 'x151',\n 'y151',\n 'z151',\n 'v151',\n 'x152',\n 'y152',\n 'z152',\n 'v152',\n 'x153',\n 'y153',\n 'z153',\n 'v153',\n 'x154',\n 'y154',\n 'z154',\n 'v154',\n 'x155',\n 'y155',\n 'z155',\n 'v155',\n 'x156',\n 'y156',\n 'z156',\n 'v156',\n 'x157',\n 'y157',\n 'z157',\n 'v157',\n 'x158',\n 'y158',\n 'z158',\n 'v158',\n 'x159',\n 'y159',\n 'z159',\n 'v159',\n 'x160',\n 'y160',\n 'z160',\n 'v160',\n 'x161',\n 'y161',\n 'z161',\n 'v161',\n 'x162',\n 'y162',\n 'z162',\n 'v162',\n 'x163',\n 'y163',\n 'z163',\n 'v163',\n 'x164',\n 'y164',\n 'z164',\n 'v164',\n 'x165',\n 'y165',\n 'z165',\n 'v165',\n 'x166',\n 'y166',\n 'z166',\n 'v166',\n 'x167',\n 'y167',\n 'z167',\n 'v167',\n 'x168',\n 'y168',\n 'z168',\n 'v168',\n 'x169',\n 'y169',\n 'z169',\n 'v169',\n 'x170',\n 'y170',\n 'z170',\n 'v170',\n 'x171',\n 'y171',\n 'z171',\n 'v171',\n 'x172',\n 'y172',\n 'z172',\n 'v172',\n 'x173',\n 'y173',\n 'z173',\n 'v173',\n 'x174',\n 'y174',\n 'z174',\n 'v174',\n 'x175',\n 'y175',\n 'z175',\n 'v175',\n 'x176',\n 'y176',\n 'z176',\n 'v176',\n 'x177',\n 'y177',\n 'z177',\n 'v177',\n 'x178',\n 'y178',\n 'z178',\n 'v178',\n 'x179',\n 'y179',\n 'z179',\n 'v179',\n 'x180',\n 'y180',\n 'z180',\n 'v180',\n 'x181',\n 'y181',\n 'z181',\n 'v181',\n 'x182',\n 'y182',\n 'z182',\n 'v182',\n 'x183',\n 'y183',\n 'z183',\n 'v183',\n 'x184',\n 'y184',\n 'z184',\n 'v184',\n 'x185',\n 'y185',\n 'z185',\n 'v185',\n 'x186',\n 'y186',\n 'z186',\n 'v186',\n 'x187',\n 'y187',\n 'z187',\n 'v187',\n 'x188',\n 'y188',\n 'z188',\n 'v188',\n 'x189',\n 'y189',\n 'z189',\n 'v189',\n 'x190',\n 'y190',\n 'z190',\n 'v190',\n 'x191',\n 'y191',\n 'z191',\n 'v191',\n 'x192',\n 'y192',\n 'z192',\n 'v192',\n 'x193',\n 'y193',\n 'z193',\n 'v193',\n 'x194',\n 'y194',\n 'z194',\n 'v194',\n 'x195',\n 'y195',\n 'z195',\n 'v195',\n 'x196',\n 'y196',\n 'z196',\n 'v196',\n 'x197',\n 'y197',\n 'z197',\n 'v197',\n 'x198',\n 'y198',\n 'z198',\n 'v198',\n 'x199',\n 'y199',\n 'z199',\n 'v199',\n 'x200',\n 'y200',\n 'z200',\n 'v200',\n 'x201',\n 'y201',\n 'z201',\n 'v201',\n 'x202',\n 'y202',\n 'z202',\n 'v202',\n 'x203',\n 'y203',\n 'z203',\n 'v203',\n 'x204',\n 'y204',\n 'z204',\n 'v204',\n 'x205',\n 'y205',\n 'z205',\n 'v205',\n 'x206',\n 'y206',\n 'z206',\n 'v206',\n 'x207',\n 'y207',\n 'z207',\n 'v207',\n 'x208',\n 'y208',\n 'z208',\n 'v208',\n 'x209',\n 'y209',\n 'z209',\n 'v209',\n 'x210',\n 'y210',\n 'z210',\n 'v210',\n 'x211',\n 'y211',\n 'z211',\n 'v211',\n 'x212',\n 'y212',\n 'z212',\n 'v212',\n 'x213',\n 'y213',\n 'z213',\n 'v213',\n 'x214',\n 'y214',\n 'z214',\n 'v214',\n 'x215',\n 'y215',\n 'z215',\n 'v215',\n 'x216',\n 'y216',\n 'z216',\n 'v216',\n 'x217',\n 'y217',\n 'z217',\n 'v217',\n 'x218',\n 'y218',\n 'z218',\n 'v218',\n 'x219',\n 'y219',\n 'z219',\n 'v219',\n 'x220',\n 'y220',\n 'z220',\n 'v220',\n 'x221',\n 'y221',\n 'z221',\n 'v221',\n 'x222',\n 'y222',\n 'z222',\n 'v222',\n 'x223',\n 'y223',\n 'z223',\n 'v223',\n 'x224',\n 'y224',\n 'z224',\n 'v224',\n 'x225',\n 'y225',\n 'z225',\n 'v225',\n 'x226',\n 'y226',\n 'z226',\n 'v226',\n 'x227',\n 'y227',\n 'z227',\n 'v227',\n 'x228',\n 'y228',\n 'z228',\n 'v228',\n 'x229',\n 'y229',\n 'z229',\n 'v229',\n 'x230',\n 'y230',\n 'z230',\n 'v230',\n 'x231',\n 'y231',\n 'z231',\n 'v231',\n 'x232',\n 'y232',\n 'z232',\n 'v232',\n 'x233',\n 'y233',\n 'z233',\n 'v233',\n 'x234',\n 'y234',\n 'z234',\n 'v234',\n 'x235',\n 'y235',\n 'z235',\n 'v235',\n 'x236',\n 'y236',\n 'z236',\n 'v236',\n 'x237',\n 'y237',\n 'z237',\n 'v237',\n 'x238',\n 'y238',\n 'z238',\n 'v238',\n 'x239',\n 'y239',\n 'z239',\n 'v239',\n 'x240',\n 'y240',\n 'z240',\n 'v240',\n 'x241',\n 'y241',\n 'z241',\n 'v241',\n 'x242',\n 'y242',\n 'z242',\n 'v242',\n 'x243',\n 'y243',\n 'z243',\n 'v243',\n 'x244',\n 'y244',\n 'z244',\n 'v244',\n 'x245',\n 'y245',\n 'z245',\n 'v245',\n 'x246',\n 'y246',\n 'z246',\n 'v246',\n 'x247',\n 'y247',\n 'z247',\n 'v247',\n 'x248',\n 'y248',\n 'z248',\n 'v248',\n 'x249',\n 'y249',\n 'z249',\n 'v249',\n 'x250',\n 'y250',\n 'z250',\n ...]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('coords.csv', mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_name = \"Awake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            # Concate rows\n",
    "            row = face_row\n",
    "            \n",
    "            # Append class name \n",
    "            row.insert(0, class_name)\n",
    "            \n",
    "            # Export to CSV\n",
    "            with open('coords.csv', mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row) \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Train Custom Model Using Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.1 Read in Collected Data and Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    class        x1        y1        z1   v1        x2        y2        z2   \n0  Drowsy  0.313424  1.206136 -0.001129  0.0  0.301826  1.163661 -0.091538  \\\n1  Drowsy  0.312936  1.174586 -0.006070  0.0  0.300156  1.123825 -0.089882   \n2  Drowsy  0.315449  1.142095 -0.009121  0.0  0.302255  1.082512 -0.091458   \n3  Drowsy  0.312416  1.122668 -0.017206  0.0  0.300815  1.062585 -0.097699   \n4  Drowsy  0.322679  1.091228 -0.018929  0.0  0.312377  1.025355 -0.099036   \n\n    v2        x3  ...      z466  v466      x467      y467      z467  v467   \n0  0.0  0.303919  ... -0.068046   0.0  0.408158  0.812459 -0.048229   0.0  \\\n1  0.0  0.303184  ... -0.060572   0.0  0.403202  0.766442 -0.040929   0.0   \n2  0.0  0.303273  ... -0.056842   0.0  0.400417  0.730013 -0.031300   0.0   \n3  0.0  0.300877  ... -0.050980   0.0  0.403654  0.715786 -0.020303   0.0   \n4  0.0  0.310536  ... -0.050241   0.0  0.408677  0.687184 -0.015059   0.0   \n\n       x468      y468      z468  v468  \n0  0.415133  0.789683 -0.050119   0.0  \n1  0.409945  0.740227 -0.042361   0.0  \n2  0.407177  0.703712 -0.032223   0.0  \n3  0.411252  0.691865 -0.021105   0.0  \n4  0.415922  0.661506 -0.015079   0.0  \n\n[5 rows x 1873 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>x1</th>\n      <th>y1</th>\n      <th>z1</th>\n      <th>v1</th>\n      <th>x2</th>\n      <th>y2</th>\n      <th>z2</th>\n      <th>v2</th>\n      <th>x3</th>\n      <th>...</th>\n      <th>z466</th>\n      <th>v466</th>\n      <th>x467</th>\n      <th>y467</th>\n      <th>z467</th>\n      <th>v467</th>\n      <th>x468</th>\n      <th>y468</th>\n      <th>z468</th>\n      <th>v468</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Drowsy</td>\n      <td>0.313424</td>\n      <td>1.206136</td>\n      <td>-0.001129</td>\n      <td>0.0</td>\n      <td>0.301826</td>\n      <td>1.163661</td>\n      <td>-0.091538</td>\n      <td>0.0</td>\n      <td>0.303919</td>\n      <td>...</td>\n      <td>-0.068046</td>\n      <td>0.0</td>\n      <td>0.408158</td>\n      <td>0.812459</td>\n      <td>-0.048229</td>\n      <td>0.0</td>\n      <td>0.415133</td>\n      <td>0.789683</td>\n      <td>-0.050119</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Drowsy</td>\n      <td>0.312936</td>\n      <td>1.174586</td>\n      <td>-0.006070</td>\n      <td>0.0</td>\n      <td>0.300156</td>\n      <td>1.123825</td>\n      <td>-0.089882</td>\n      <td>0.0</td>\n      <td>0.303184</td>\n      <td>...</td>\n      <td>-0.060572</td>\n      <td>0.0</td>\n      <td>0.403202</td>\n      <td>0.766442</td>\n      <td>-0.040929</td>\n      <td>0.0</td>\n      <td>0.409945</td>\n      <td>0.740227</td>\n      <td>-0.042361</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Drowsy</td>\n      <td>0.315449</td>\n      <td>1.142095</td>\n      <td>-0.009121</td>\n      <td>0.0</td>\n      <td>0.302255</td>\n      <td>1.082512</td>\n      <td>-0.091458</td>\n      <td>0.0</td>\n      <td>0.303273</td>\n      <td>...</td>\n      <td>-0.056842</td>\n      <td>0.0</td>\n      <td>0.400417</td>\n      <td>0.730013</td>\n      <td>-0.031300</td>\n      <td>0.0</td>\n      <td>0.407177</td>\n      <td>0.703712</td>\n      <td>-0.032223</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Drowsy</td>\n      <td>0.312416</td>\n      <td>1.122668</td>\n      <td>-0.017206</td>\n      <td>0.0</td>\n      <td>0.300815</td>\n      <td>1.062585</td>\n      <td>-0.097699</td>\n      <td>0.0</td>\n      <td>0.300877</td>\n      <td>...</td>\n      <td>-0.050980</td>\n      <td>0.0</td>\n      <td>0.403654</td>\n      <td>0.715786</td>\n      <td>-0.020303</td>\n      <td>0.0</td>\n      <td>0.411252</td>\n      <td>0.691865</td>\n      <td>-0.021105</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Drowsy</td>\n      <td>0.322679</td>\n      <td>1.091228</td>\n      <td>-0.018929</td>\n      <td>0.0</td>\n      <td>0.312377</td>\n      <td>1.025355</td>\n      <td>-0.099036</td>\n      <td>0.0</td>\n      <td>0.310536</td>\n      <td>...</td>\n      <td>-0.050241</td>\n      <td>0.0</td>\n      <td>0.408677</td>\n      <td>0.687184</td>\n      <td>-0.015059</td>\n      <td>0.0</td>\n      <td>0.415922</td>\n      <td>0.661506</td>\n      <td>-0.015079</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 1873 columns</p>\n</div>"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               class   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167    0.0  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912    0.0   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433    0.0   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129    0.0   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667    0.0   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     x1   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.645387  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.645132   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.641902   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.646801   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.653554   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     y1   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.650755  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.655302   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.668817   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.677566   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.676516   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     z1   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.015320  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.015248   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.009431   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.008005   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.005254   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                v1   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.0  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.0   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.0   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.0   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.0   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     x2   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.621671  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.620738   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.615813   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.620470   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.624486   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     y2   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.652679  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.656808   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.669113   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.675746   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.672873   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     z2   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.056662  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.057958   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.051343   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.050122   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.045415   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                v2   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.0  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.0   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.0   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.0   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.0   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     x3   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.632237  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.631653   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.628261   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.632790   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.638457   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  ...  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  ...   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  ...   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  ...   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  ...   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   z466   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.748308  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.670634   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.491578   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.650842   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.574013   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   v466   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.000022  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.000029   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.000032   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.000032   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.000033   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   x467   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.630885  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.603360   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.593865   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.591643   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.573739   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   y467   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  2.605875  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  2.615064   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  2.637984   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  2.657654   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  2.662299   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   z467   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.010930  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912 -0.020720   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433 -0.097980   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129 -0.032136   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667 -0.063027   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   v467   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.000047  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.000047   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.000046   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.000044   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.000044   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   x468   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.490275  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.467432   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.462530   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.461757   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.443591   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   y468   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  2.589485  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  2.591561   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  2.605663   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  2.624578   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  2.626521   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   z468   \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.207700  \\\n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.107094   \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433 -0.076409   \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.061801   \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667 -0.047177   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   v468  \nDrowsy 0.687386 0.773003 -0.021717 0.0 0.689304 0.738972 -0.045031 0.0 0.690003 0.750166 -0.023985 0.0 0.689463 0.696674 -0.034669 0.0 0.690933 0.726281 -0.048150 0.0 0.693819 0.709804 -0.045428 0.0 0.701537 0.670910 -0.024610 0.0 0.647909 0.653996 0.012467 0.0 0.706807 0.639019 -0.019600 0.0 0.709267 0.622429 -0.021538 0.0 0.719038 0.565764 -0.015762 0.0 0.686758 0.778760 -0.020474 0.0 0.686476 0.783095 -0.017869 0.0 0.686637 0.784639 -0.014392 0.0 0.684884 0.804993 -0.008752 0.0 0.684020 0.810829 -0.009973 0.0 0.683042 0.817692 -0.011353 0.0 0.682064 0.825005 -0.009423 0.0 0.680841 0.834297 -0.001757 0.0 0.689031 0.745434 -0.040824 0.0 0.682250 0.742388 -0.029260 0.0 0.633112 0.602848 0.051199 0.0 0.669431 0.668998  0.002681 0.0 0.661625 0.668027  0.004077 0.0 0.654140 0.665674  0.006980 0.0 0.644390 0.656814 0.014275 0.0 0.675905 0.667784  0.002875 0.0 0.662172 0.635898 -0.000308 0.0 0.670388 0.639389 -0.000980 0.0 0.654003 0.635290  0.003330 0.0 0.648281 0.638326  0.007185 0.0 0.638026 0.662648 0.019058 0.0 0.649238 0.837201 0.012167  0.000065  \n       0.688338 0.778541 -0.021646 0.0 0.689500 0.744254 -0.045753 0.0 0.690106 0.755774 -0.023895 0.0 0.689838 0.701467 -0.035542 0.0 0.691167 0.731272 -0.049021 0.0 0.694161 0.714582 -0.046358 0.0 0.702112 0.675536 -0.025472 0.0 0.647654 0.658646 0.012249 0.0 0.707864 0.641998 -0.020565 0.0 0.710619 0.624543 -0.022841 0.0 0.720322 0.566856 -0.018043 0.0 0.687840 0.784262 -0.020391 0.0 0.687665 0.788536 -0.017679 0.0 0.687916 0.789971 -0.014056 0.0 0.685580 0.808200 -0.009432 0.0 0.684696 0.813806 -0.010602 0.0 0.683710 0.820524 -0.011827 0.0 0.682768 0.827852 -0.009814 0.0 0.681451 0.837833 -0.001488 0.0 0.689138 0.750964 -0.041323 0.0 0.682503 0.747907 -0.029403 0.0 0.631840 0.605789 0.051652 0.0 0.669150 0.674410  0.002111 0.0 0.661292 0.673219  0.003664 0.0 0.653862 0.670638  0.006784 0.0 0.644114 0.661430 0.014299 0.0 0.675759 0.673203  0.002488 0.0 0.662656 0.640408 -0.001076 0.0 0.670874 0.643974 -0.001851 0.0 0.654383 0.639754  0.002817 0.0 0.648439 0.642799  0.006734 0.0 0.637609 0.667229 0.019339 0.0 0.650216 0.841583 0.012912  0.000070  \n       0.693869 0.796166 -0.020136 0.0 0.695531 0.763532 -0.046625 0.0 0.695195 0.773912 -0.023703 0.0 0.693447 0.718770 -0.038013 0.0 0.696934 0.750254 -0.050329 0.0 0.699069 0.732667 -0.048163 0.0 0.704313 0.690582 -0.028273 0.0 0.644860 0.672629 0.006572 0.0 0.708853 0.654046 -0.024190 0.0 0.711092 0.635855 -0.027262 0.0 0.717986 0.574933 -0.025198 0.0 0.693372 0.801924 -0.018586 0.0 0.693034 0.806069 -0.015727 0.0 0.693015 0.807184 -0.011938 0.0 0.690432 0.826450 -0.006685 0.0 0.689813 0.832374 -0.007723 0.0 0.689057 0.839266 -0.008796 0.0 0.688217 0.846651 -0.006572 0.0 0.687135 0.855709  0.002123 0.0 0.695010 0.769955 -0.041877 0.0 0.687300 0.766581 -0.029980 0.0 0.625059 0.615600 0.042640 0.0 0.668576 0.688530 -0.002276 0.0 0.660332 0.687675 -0.001106 0.0 0.652343 0.685293  0.001679 0.0 0.641321 0.675331 0.008651 0.0 0.675403 0.687067 -0.001529 0.0 0.660239 0.652756 -0.006838 0.0 0.669112 0.656453 -0.007203 0.0 0.651349 0.652271 -0.003169 0.0 0.645047 0.655594  0.000706 0.0 0.635029 0.681513 0.013832 0.0 0.655057 0.859875 0.016433  0.000074  \n       0.697115 0.806928 -0.019345 0.0 0.699614 0.776587 -0.046493 0.0 0.699148 0.785749 -0.023113 0.0 0.698055 0.731064 -0.038532 0.0 0.701201 0.763422 -0.050364 0.0 0.703551 0.745587 -0.048452 0.0 0.709370 0.702643 -0.029233 0.0 0.649798 0.681404 0.005249 0.0 0.714071 0.666325 -0.025845 0.0 0.716489 0.648258 -0.029328 0.0 0.724561 0.585928 -0.028321 0.0 0.696515 0.812660 -0.017715 0.0 0.696154 0.816754 -0.014726 0.0 0.696160 0.817886 -0.010836 0.0 0.694114 0.835509 -0.005787 0.0 0.693426 0.841557 -0.006770 0.0 0.692586 0.848590 -0.007705 0.0 0.691683 0.855978 -0.005299 0.0 0.690519 0.865144  0.003785 0.0 0.699043 0.782700 -0.041612 0.0 0.691224 0.778645 -0.029695 0.0 0.630708 0.622463 0.040291 0.0 0.673511 0.697800 -0.003379 0.0 0.665230 0.696693 -0.002239 0.0 0.657247 0.694125  0.000462 0.0 0.646120 0.683922 0.007329 0.0 0.680321 0.696740 -0.002536 0.0 0.665020 0.663271 -0.008474 0.0 0.673893 0.667008 -0.008761 0.0 0.656190 0.662391 -0.004829 0.0 0.649966 0.665219 -0.000930 0.0 0.639661 0.689809 0.012619 0.0 0.658282 0.868710 0.018129  0.000073  \n       0.698107 0.811549 -0.018575 0.0 0.703640 0.780797 -0.045482 0.0 0.701677 0.789931 -0.022308 0.0 0.703491 0.734627 -0.038125 0.0 0.705833 0.767685 -0.049329 0.0 0.708623 0.749752 -0.047471 0.0 0.715208 0.706375 -0.028714 0.0 0.656338 0.680646 0.002649 0.0 0.721013 0.670428 -0.025423 0.0 0.724225 0.652446 -0.028917 0.0 0.734695 0.589961 -0.028093 0.0 0.697154 0.817158 -0.016987 0.0 0.696427 0.820998 -0.014057 0.0 0.696156 0.821880 -0.010220 0.0 0.693993 0.835612 -0.006556 0.0 0.693106 0.841363 -0.007500 0.0 0.691991 0.848158 -0.008464 0.0 0.690734 0.855345 -0.006243 0.0 0.688792 0.864909  0.002785 0.0 0.702493 0.786833 -0.040639 0.0 0.694748 0.782199 -0.029111 0.0 0.636908 0.619533 0.035553 0.0 0.678919 0.699176 -0.004922 0.0 0.670715 0.697378 -0.004144 0.0 0.662946 0.694134 -0.001811 0.0 0.652489 0.683238 0.004586 0.0 0.685892 0.698643 -0.003725 0.0 0.672703 0.663683 -0.010238 0.0 0.681361 0.667904 -0.010122 0.0 0.663970 0.662481 -0.007022 0.0 0.657543 0.665036 -0.003479 0.0 0.645401 0.688686 0.009599 0.0 0.656139 0.868328 0.015667  0.000074  \n\n[5 rows x 1873 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th>class</th>\n      <th>x1</th>\n      <th>y1</th>\n      <th>z1</th>\n      <th>v1</th>\n      <th>x2</th>\n      <th>y2</th>\n      <th>z2</th>\n      <th>v2</th>\n      <th>x3</th>\n      <th>...</th>\n      <th>z466</th>\n      <th>v466</th>\n      <th>x467</th>\n      <th>y467</th>\n      <th>z467</th>\n      <th>v467</th>\n      <th>x468</th>\n      <th>y468</th>\n      <th>z468</th>\n      <th>v468</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">Drowsy</th>\n      <th>0.687386</th>\n      <th>0.773003</th>\n      <th>-0.021717</th>\n      <th>0.0</th>\n      <th>0.689304</th>\n      <th>0.738972</th>\n      <th>-0.045031</th>\n      <th>0.0</th>\n      <th>0.690003</th>\n      <th>0.750166</th>\n      <th>-0.023985</th>\n      <th>0.0</th>\n      <th>0.689463</th>\n      <th>0.696674</th>\n      <th>-0.034669</th>\n      <th>0.0</th>\n      <th>0.690933</th>\n      <th>0.726281</th>\n      <th>-0.048150</th>\n      <th>0.0</th>\n      <th>0.693819</th>\n      <th>0.709804</th>\n      <th>-0.045428</th>\n      <th>0.0</th>\n      <th>0.701537</th>\n      <th>0.670910</th>\n      <th>-0.024610</th>\n      <th>0.0</th>\n      <th>0.647909</th>\n      <th>0.653996</th>\n      <th>0.012467</th>\n      <th>0.0</th>\n      <th>0.706807</th>\n      <th>0.639019</th>\n      <th>-0.019600</th>\n      <th>0.0</th>\n      <th>0.709267</th>\n      <th>0.622429</th>\n      <th>-0.021538</th>\n      <th>0.0</th>\n      <th>0.719038</th>\n      <th>0.565764</th>\n      <th>-0.015762</th>\n      <th>0.0</th>\n      <th>0.686758</th>\n      <th>0.778760</th>\n      <th>-0.020474</th>\n      <th>0.0</th>\n      <th>0.686476</th>\n      <th>0.783095</th>\n      <th>-0.017869</th>\n      <th>0.0</th>\n      <th>0.686637</th>\n      <th>0.784639</th>\n      <th>-0.014392</th>\n      <th>0.0</th>\n      <th>0.684884</th>\n      <th>0.804993</th>\n      <th>-0.008752</th>\n      <th>0.0</th>\n      <th>0.684020</th>\n      <th>0.810829</th>\n      <th>-0.009973</th>\n      <th>0.0</th>\n      <th>0.683042</th>\n      <th>0.817692</th>\n      <th>-0.011353</th>\n      <th>0.0</th>\n      <th>0.682064</th>\n      <th>0.825005</th>\n      <th>-0.009423</th>\n      <th>0.0</th>\n      <th>0.680841</th>\n      <th>0.834297</th>\n      <th>-0.001757</th>\n      <th>0.0</th>\n      <th>0.689031</th>\n      <th>0.745434</th>\n      <th>-0.040824</th>\n      <th>0.0</th>\n      <th>0.682250</th>\n      <th>0.742388</th>\n      <th>-0.029260</th>\n      <th>0.0</th>\n      <th>0.633112</th>\n      <th>0.602848</th>\n      <th>0.051199</th>\n      <th>0.0</th>\n      <th>0.669431</th>\n      <th>0.668998</th>\n      <th>0.002681</th>\n      <th>0.0</th>\n      <th>0.661625</th>\n      <th>0.668027</th>\n      <th>0.004077</th>\n      <th>0.0</th>\n      <th>0.654140</th>\n      <th>0.665674</th>\n      <th>0.006980</th>\n      <th>0.0</th>\n      <th>0.644390</th>\n      <th>0.656814</th>\n      <th>0.014275</th>\n      <th>0.0</th>\n      <th>0.675905</th>\n      <th>0.667784</th>\n      <th>0.002875</th>\n      <th>0.0</th>\n      <th>0.662172</th>\n      <th>0.635898</th>\n      <th>-0.000308</th>\n      <th>0.0</th>\n      <th>0.670388</th>\n      <th>0.639389</th>\n      <th>-0.000980</th>\n      <th>0.0</th>\n      <th>0.654003</th>\n      <th>0.635290</th>\n      <th>0.003330</th>\n      <th>0.0</th>\n      <th>0.648281</th>\n      <th>0.638326</th>\n      <th>0.007185</th>\n      <th>0.0</th>\n      <th>0.638026</th>\n      <th>0.662648</th>\n      <th>0.019058</th>\n      <th>0.0</th>\n      <th>0.649238</th>\n      <th>0.837201</th>\n      <th>0.012167</th>\n      <td>0.0</td>\n      <td>0.645387</td>\n      <td>0.650755</td>\n      <td>0.015320</td>\n      <td>0.0</td>\n      <td>0.621671</td>\n      <td>0.652679</td>\n      <td>0.056662</td>\n      <td>0.0</td>\n      <td>0.632237</td>\n      <td>...</td>\n      <td>0.748308</td>\n      <td>0.000022</td>\n      <td>0.630885</td>\n      <td>2.605875</td>\n      <td>0.010930</td>\n      <td>0.000047</td>\n      <td>0.490275</td>\n      <td>2.589485</td>\n      <td>0.207700</td>\n      <td>0.000065</td>\n    </tr>\n    <tr>\n      <th>0.688338</th>\n      <th>0.778541</th>\n      <th>-0.021646</th>\n      <th>0.0</th>\n      <th>0.689500</th>\n      <th>0.744254</th>\n      <th>-0.045753</th>\n      <th>0.0</th>\n      <th>0.690106</th>\n      <th>0.755774</th>\n      <th>-0.023895</th>\n      <th>0.0</th>\n      <th>0.689838</th>\n      <th>0.701467</th>\n      <th>-0.035542</th>\n      <th>0.0</th>\n      <th>0.691167</th>\n      <th>0.731272</th>\n      <th>-0.049021</th>\n      <th>0.0</th>\n      <th>0.694161</th>\n      <th>0.714582</th>\n      <th>-0.046358</th>\n      <th>0.0</th>\n      <th>0.702112</th>\n      <th>0.675536</th>\n      <th>-0.025472</th>\n      <th>0.0</th>\n      <th>0.647654</th>\n      <th>0.658646</th>\n      <th>0.012249</th>\n      <th>0.0</th>\n      <th>0.707864</th>\n      <th>0.641998</th>\n      <th>-0.020565</th>\n      <th>0.0</th>\n      <th>0.710619</th>\n      <th>0.624543</th>\n      <th>-0.022841</th>\n      <th>0.0</th>\n      <th>0.720322</th>\n      <th>0.566856</th>\n      <th>-0.018043</th>\n      <th>0.0</th>\n      <th>0.687840</th>\n      <th>0.784262</th>\n      <th>-0.020391</th>\n      <th>0.0</th>\n      <th>0.687665</th>\n      <th>0.788536</th>\n      <th>-0.017679</th>\n      <th>0.0</th>\n      <th>0.687916</th>\n      <th>0.789971</th>\n      <th>-0.014056</th>\n      <th>0.0</th>\n      <th>0.685580</th>\n      <th>0.808200</th>\n      <th>-0.009432</th>\n      <th>0.0</th>\n      <th>0.684696</th>\n      <th>0.813806</th>\n      <th>-0.010602</th>\n      <th>0.0</th>\n      <th>0.683710</th>\n      <th>0.820524</th>\n      <th>-0.011827</th>\n      <th>0.0</th>\n      <th>0.682768</th>\n      <th>0.827852</th>\n      <th>-0.009814</th>\n      <th>0.0</th>\n      <th>0.681451</th>\n      <th>0.837833</th>\n      <th>-0.001488</th>\n      <th>0.0</th>\n      <th>0.689138</th>\n      <th>0.750964</th>\n      <th>-0.041323</th>\n      <th>0.0</th>\n      <th>0.682503</th>\n      <th>0.747907</th>\n      <th>-0.029403</th>\n      <th>0.0</th>\n      <th>0.631840</th>\n      <th>0.605789</th>\n      <th>0.051652</th>\n      <th>0.0</th>\n      <th>0.669150</th>\n      <th>0.674410</th>\n      <th>0.002111</th>\n      <th>0.0</th>\n      <th>0.661292</th>\n      <th>0.673219</th>\n      <th>0.003664</th>\n      <th>0.0</th>\n      <th>0.653862</th>\n      <th>0.670638</th>\n      <th>0.006784</th>\n      <th>0.0</th>\n      <th>0.644114</th>\n      <th>0.661430</th>\n      <th>0.014299</th>\n      <th>0.0</th>\n      <th>0.675759</th>\n      <th>0.673203</th>\n      <th>0.002488</th>\n      <th>0.0</th>\n      <th>0.662656</th>\n      <th>0.640408</th>\n      <th>-0.001076</th>\n      <th>0.0</th>\n      <th>0.670874</th>\n      <th>0.643974</th>\n      <th>-0.001851</th>\n      <th>0.0</th>\n      <th>0.654383</th>\n      <th>0.639754</th>\n      <th>0.002817</th>\n      <th>0.0</th>\n      <th>0.648439</th>\n      <th>0.642799</th>\n      <th>0.006734</th>\n      <th>0.0</th>\n      <th>0.637609</th>\n      <th>0.667229</th>\n      <th>0.019339</th>\n      <th>0.0</th>\n      <th>0.650216</th>\n      <th>0.841583</th>\n      <th>0.012912</th>\n      <td>0.0</td>\n      <td>0.645132</td>\n      <td>0.655302</td>\n      <td>0.015248</td>\n      <td>0.0</td>\n      <td>0.620738</td>\n      <td>0.656808</td>\n      <td>0.057958</td>\n      <td>0.0</td>\n      <td>0.631653</td>\n      <td>...</td>\n      <td>0.670634</td>\n      <td>0.000029</td>\n      <td>0.603360</td>\n      <td>2.615064</td>\n      <td>-0.020720</td>\n      <td>0.000047</td>\n      <td>0.467432</td>\n      <td>2.591561</td>\n      <td>0.107094</td>\n      <td>0.000070</td>\n    </tr>\n    <tr>\n      <th>0.693869</th>\n      <th>0.796166</th>\n      <th>-0.020136</th>\n      <th>0.0</th>\n      <th>0.695531</th>\n      <th>0.763532</th>\n      <th>-0.046625</th>\n      <th>0.0</th>\n      <th>0.695195</th>\n      <th>0.773912</th>\n      <th>-0.023703</th>\n      <th>0.0</th>\n      <th>0.693447</th>\n      <th>0.718770</th>\n      <th>-0.038013</th>\n      <th>0.0</th>\n      <th>0.696934</th>\n      <th>0.750254</th>\n      <th>-0.050329</th>\n      <th>0.0</th>\n      <th>0.699069</th>\n      <th>0.732667</th>\n      <th>-0.048163</th>\n      <th>0.0</th>\n      <th>0.704313</th>\n      <th>0.690582</th>\n      <th>-0.028273</th>\n      <th>0.0</th>\n      <th>0.644860</th>\n      <th>0.672629</th>\n      <th>0.006572</th>\n      <th>0.0</th>\n      <th>0.708853</th>\n      <th>0.654046</th>\n      <th>-0.024190</th>\n      <th>0.0</th>\n      <th>0.711092</th>\n      <th>0.635855</th>\n      <th>-0.027262</th>\n      <th>0.0</th>\n      <th>0.717986</th>\n      <th>0.574933</th>\n      <th>-0.025198</th>\n      <th>0.0</th>\n      <th>0.693372</th>\n      <th>0.801924</th>\n      <th>-0.018586</th>\n      <th>0.0</th>\n      <th>0.693034</th>\n      <th>0.806069</th>\n      <th>-0.015727</th>\n      <th>0.0</th>\n      <th>0.693015</th>\n      <th>0.807184</th>\n      <th>-0.011938</th>\n      <th>0.0</th>\n      <th>0.690432</th>\n      <th>0.826450</th>\n      <th>-0.006685</th>\n      <th>0.0</th>\n      <th>0.689813</th>\n      <th>0.832374</th>\n      <th>-0.007723</th>\n      <th>0.0</th>\n      <th>0.689057</th>\n      <th>0.839266</th>\n      <th>-0.008796</th>\n      <th>0.0</th>\n      <th>0.688217</th>\n      <th>0.846651</th>\n      <th>-0.006572</th>\n      <th>0.0</th>\n      <th>0.687135</th>\n      <th>0.855709</th>\n      <th>0.002123</th>\n      <th>0.0</th>\n      <th>0.695010</th>\n      <th>0.769955</th>\n      <th>-0.041877</th>\n      <th>0.0</th>\n      <th>0.687300</th>\n      <th>0.766581</th>\n      <th>-0.029980</th>\n      <th>0.0</th>\n      <th>0.625059</th>\n      <th>0.615600</th>\n      <th>0.042640</th>\n      <th>0.0</th>\n      <th>0.668576</th>\n      <th>0.688530</th>\n      <th>-0.002276</th>\n      <th>0.0</th>\n      <th>0.660332</th>\n      <th>0.687675</th>\n      <th>-0.001106</th>\n      <th>0.0</th>\n      <th>0.652343</th>\n      <th>0.685293</th>\n      <th>0.001679</th>\n      <th>0.0</th>\n      <th>0.641321</th>\n      <th>0.675331</th>\n      <th>0.008651</th>\n      <th>0.0</th>\n      <th>0.675403</th>\n      <th>0.687067</th>\n      <th>-0.001529</th>\n      <th>0.0</th>\n      <th>0.660239</th>\n      <th>0.652756</th>\n      <th>-0.006838</th>\n      <th>0.0</th>\n      <th>0.669112</th>\n      <th>0.656453</th>\n      <th>-0.007203</th>\n      <th>0.0</th>\n      <th>0.651349</th>\n      <th>0.652271</th>\n      <th>-0.003169</th>\n      <th>0.0</th>\n      <th>0.645047</th>\n      <th>0.655594</th>\n      <th>0.000706</th>\n      <th>0.0</th>\n      <th>0.635029</th>\n      <th>0.681513</th>\n      <th>0.013832</th>\n      <th>0.0</th>\n      <th>0.655057</th>\n      <th>0.859875</th>\n      <th>0.016433</th>\n      <td>0.0</td>\n      <td>0.641902</td>\n      <td>0.668817</td>\n      <td>0.009431</td>\n      <td>0.0</td>\n      <td>0.615813</td>\n      <td>0.669113</td>\n      <td>0.051343</td>\n      <td>0.0</td>\n      <td>0.628261</td>\n      <td>...</td>\n      <td>0.491578</td>\n      <td>0.000032</td>\n      <td>0.593865</td>\n      <td>2.637984</td>\n      <td>-0.097980</td>\n      <td>0.000046</td>\n      <td>0.462530</td>\n      <td>2.605663</td>\n      <td>-0.076409</td>\n      <td>0.000074</td>\n    </tr>\n    <tr>\n      <th>0.697115</th>\n      <th>0.806928</th>\n      <th>-0.019345</th>\n      <th>0.0</th>\n      <th>0.699614</th>\n      <th>0.776587</th>\n      <th>-0.046493</th>\n      <th>0.0</th>\n      <th>0.699148</th>\n      <th>0.785749</th>\n      <th>-0.023113</th>\n      <th>0.0</th>\n      <th>0.698055</th>\n      <th>0.731064</th>\n      <th>-0.038532</th>\n      <th>0.0</th>\n      <th>0.701201</th>\n      <th>0.763422</th>\n      <th>-0.050364</th>\n      <th>0.0</th>\n      <th>0.703551</th>\n      <th>0.745587</th>\n      <th>-0.048452</th>\n      <th>0.0</th>\n      <th>0.709370</th>\n      <th>0.702643</th>\n      <th>-0.029233</th>\n      <th>0.0</th>\n      <th>0.649798</th>\n      <th>0.681404</th>\n      <th>0.005249</th>\n      <th>0.0</th>\n      <th>0.714071</th>\n      <th>0.666325</th>\n      <th>-0.025845</th>\n      <th>0.0</th>\n      <th>0.716489</th>\n      <th>0.648258</th>\n      <th>-0.029328</th>\n      <th>0.0</th>\n      <th>0.724561</th>\n      <th>0.585928</th>\n      <th>-0.028321</th>\n      <th>0.0</th>\n      <th>0.696515</th>\n      <th>0.812660</th>\n      <th>-0.017715</th>\n      <th>0.0</th>\n      <th>0.696154</th>\n      <th>0.816754</th>\n      <th>-0.014726</th>\n      <th>0.0</th>\n      <th>0.696160</th>\n      <th>0.817886</th>\n      <th>-0.010836</th>\n      <th>0.0</th>\n      <th>0.694114</th>\n      <th>0.835509</th>\n      <th>-0.005787</th>\n      <th>0.0</th>\n      <th>0.693426</th>\n      <th>0.841557</th>\n      <th>-0.006770</th>\n      <th>0.0</th>\n      <th>0.692586</th>\n      <th>0.848590</th>\n      <th>-0.007705</th>\n      <th>0.0</th>\n      <th>0.691683</th>\n      <th>0.855978</th>\n      <th>-0.005299</th>\n      <th>0.0</th>\n      <th>0.690519</th>\n      <th>0.865144</th>\n      <th>0.003785</th>\n      <th>0.0</th>\n      <th>0.699043</th>\n      <th>0.782700</th>\n      <th>-0.041612</th>\n      <th>0.0</th>\n      <th>0.691224</th>\n      <th>0.778645</th>\n      <th>-0.029695</th>\n      <th>0.0</th>\n      <th>0.630708</th>\n      <th>0.622463</th>\n      <th>0.040291</th>\n      <th>0.0</th>\n      <th>0.673511</th>\n      <th>0.697800</th>\n      <th>-0.003379</th>\n      <th>0.0</th>\n      <th>0.665230</th>\n      <th>0.696693</th>\n      <th>-0.002239</th>\n      <th>0.0</th>\n      <th>0.657247</th>\n      <th>0.694125</th>\n      <th>0.000462</th>\n      <th>0.0</th>\n      <th>0.646120</th>\n      <th>0.683922</th>\n      <th>0.007329</th>\n      <th>0.0</th>\n      <th>0.680321</th>\n      <th>0.696740</th>\n      <th>-0.002536</th>\n      <th>0.0</th>\n      <th>0.665020</th>\n      <th>0.663271</th>\n      <th>-0.008474</th>\n      <th>0.0</th>\n      <th>0.673893</th>\n      <th>0.667008</th>\n      <th>-0.008761</th>\n      <th>0.0</th>\n      <th>0.656190</th>\n      <th>0.662391</th>\n      <th>-0.004829</th>\n      <th>0.0</th>\n      <th>0.649966</th>\n      <th>0.665219</th>\n      <th>-0.000930</th>\n      <th>0.0</th>\n      <th>0.639661</th>\n      <th>0.689809</th>\n      <th>0.012619</th>\n      <th>0.0</th>\n      <th>0.658282</th>\n      <th>0.868710</th>\n      <th>0.018129</th>\n      <td>0.0</td>\n      <td>0.646801</td>\n      <td>0.677566</td>\n      <td>0.008005</td>\n      <td>0.0</td>\n      <td>0.620470</td>\n      <td>0.675746</td>\n      <td>0.050122</td>\n      <td>0.0</td>\n      <td>0.632790</td>\n      <td>...</td>\n      <td>0.650842</td>\n      <td>0.000032</td>\n      <td>0.591643</td>\n      <td>2.657654</td>\n      <td>-0.032136</td>\n      <td>0.000044</td>\n      <td>0.461757</td>\n      <td>2.624578</td>\n      <td>0.061801</td>\n      <td>0.000073</td>\n    </tr>\n    <tr>\n      <th>0.698107</th>\n      <th>0.811549</th>\n      <th>-0.018575</th>\n      <th>0.0</th>\n      <th>0.703640</th>\n      <th>0.780797</th>\n      <th>-0.045482</th>\n      <th>0.0</th>\n      <th>0.701677</th>\n      <th>0.789931</th>\n      <th>-0.022308</th>\n      <th>0.0</th>\n      <th>0.703491</th>\n      <th>0.734627</th>\n      <th>-0.038125</th>\n      <th>0.0</th>\n      <th>0.705833</th>\n      <th>0.767685</th>\n      <th>-0.049329</th>\n      <th>0.0</th>\n      <th>0.708623</th>\n      <th>0.749752</th>\n      <th>-0.047471</th>\n      <th>0.0</th>\n      <th>0.715208</th>\n      <th>0.706375</th>\n      <th>-0.028714</th>\n      <th>0.0</th>\n      <th>0.656338</th>\n      <th>0.680646</th>\n      <th>0.002649</th>\n      <th>0.0</th>\n      <th>0.721013</th>\n      <th>0.670428</th>\n      <th>-0.025423</th>\n      <th>0.0</th>\n      <th>0.724225</th>\n      <th>0.652446</th>\n      <th>-0.028917</th>\n      <th>0.0</th>\n      <th>0.734695</th>\n      <th>0.589961</th>\n      <th>-0.028093</th>\n      <th>0.0</th>\n      <th>0.697154</th>\n      <th>0.817158</th>\n      <th>-0.016987</th>\n      <th>0.0</th>\n      <th>0.696427</th>\n      <th>0.820998</th>\n      <th>-0.014057</th>\n      <th>0.0</th>\n      <th>0.696156</th>\n      <th>0.821880</th>\n      <th>-0.010220</th>\n      <th>0.0</th>\n      <th>0.693993</th>\n      <th>0.835612</th>\n      <th>-0.006556</th>\n      <th>0.0</th>\n      <th>0.693106</th>\n      <th>0.841363</th>\n      <th>-0.007500</th>\n      <th>0.0</th>\n      <th>0.691991</th>\n      <th>0.848158</th>\n      <th>-0.008464</th>\n      <th>0.0</th>\n      <th>0.690734</th>\n      <th>0.855345</th>\n      <th>-0.006243</th>\n      <th>0.0</th>\n      <th>0.688792</th>\n      <th>0.864909</th>\n      <th>0.002785</th>\n      <th>0.0</th>\n      <th>0.702493</th>\n      <th>0.786833</th>\n      <th>-0.040639</th>\n      <th>0.0</th>\n      <th>0.694748</th>\n      <th>0.782199</th>\n      <th>-0.029111</th>\n      <th>0.0</th>\n      <th>0.636908</th>\n      <th>0.619533</th>\n      <th>0.035553</th>\n      <th>0.0</th>\n      <th>0.678919</th>\n      <th>0.699176</th>\n      <th>-0.004922</th>\n      <th>0.0</th>\n      <th>0.670715</th>\n      <th>0.697378</th>\n      <th>-0.004144</th>\n      <th>0.0</th>\n      <th>0.662946</th>\n      <th>0.694134</th>\n      <th>-0.001811</th>\n      <th>0.0</th>\n      <th>0.652489</th>\n      <th>0.683238</th>\n      <th>0.004586</th>\n      <th>0.0</th>\n      <th>0.685892</th>\n      <th>0.698643</th>\n      <th>-0.003725</th>\n      <th>0.0</th>\n      <th>0.672703</th>\n      <th>0.663683</th>\n      <th>-0.010238</th>\n      <th>0.0</th>\n      <th>0.681361</th>\n      <th>0.667904</th>\n      <th>-0.010122</th>\n      <th>0.0</th>\n      <th>0.663970</th>\n      <th>0.662481</th>\n      <th>-0.007022</th>\n      <th>0.0</th>\n      <th>0.657543</th>\n      <th>0.665036</th>\n      <th>-0.003479</th>\n      <th>0.0</th>\n      <th>0.645401</th>\n      <th>0.688686</th>\n      <th>0.009599</th>\n      <th>0.0</th>\n      <th>0.656139</th>\n      <th>0.868328</th>\n      <th>0.015667</th>\n      <td>0.0</td>\n      <td>0.653554</td>\n      <td>0.676516</td>\n      <td>0.005254</td>\n      <td>0.0</td>\n      <td>0.624486</td>\n      <td>0.672873</td>\n      <td>0.045415</td>\n      <td>0.0</td>\n      <td>0.638457</td>\n      <td>...</td>\n      <td>0.574013</td>\n      <td>0.000033</td>\n      <td>0.573739</td>\n      <td>2.662299</td>\n      <td>-0.063027</td>\n      <td>0.000044</td>\n      <td>0.443591</td>\n      <td>2.626521</td>\n      <td>-0.047177</td>\n      <td>0.000074</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 1873 columns</p>\n</div>"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[df['class']=='Sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1) # features\n",
    "y = df['class'] # target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "40     Drowsy\n44     Drowsy\n304     Awake\n59     Drowsy\n267     Awake\n        ...  \n162    Drowsy\n383     Awake\n253     Awake\n133    Drowsy\n262     Awake\nName: class, Length: 118, dtype: object"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Train Machine Learning Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'lr': Pipeline(steps=[('standardscaler', StandardScaler()),\n                 ('logisticregression', LogisticRegression())]),\n 'rc': Pipeline(steps=[('standardscaler', StandardScaler()),\n                 ('ridgeclassifier', RidgeClassifier())]),\n 'rf': Pipeline(steps=[('standardscaler', StandardScaler()),\n                 ('randomforestclassifier', RandomForestClassifier())]),\n 'gb': Pipeline(steps=[('standardscaler', StandardScaler()),\n                 ('gradientboostingclassifier', GradientBoostingClassifier())])}"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Drowsy', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Awake', 'Awake',\n       'Awake', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake',\n       'Awake', 'Drowsy', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Awake',\n       'Awake', 'Awake', 'Awake', 'Drowsy', 'Drowsy', 'Awake', 'Awake',\n       'Drowsy', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Drowsy',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Drowsy', 'Awake',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Awake',\n       'Drowsy', 'Drowsy', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Drowsy',\n       'Drowsy', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Drowsy', 'Drowsy', 'Awake', 'Drowsy',\n       'Drowsy', 'Awake', 'Awake', 'Awake', 'Awake', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Drowsy', 'Drowsy', 'Awake', 'Awake', 'Drowsy', 'Awake'],\n      dtype='<U6')"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rc'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.3 Evaluate and Serialize Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score # Accuracy metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.0\n",
      "rc 0.9830508474576272\n",
      "rf 1.0\n",
      "gb 1.0\n"
     ]
    }
   ],
   "source": [
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Drowsy', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Awake', 'Awake',\n       'Awake', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake',\n       'Awake', 'Drowsy', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Awake',\n       'Awake', 'Awake', 'Awake', 'Drowsy', 'Drowsy', 'Awake', 'Awake',\n       'Drowsy', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Drowsy',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Drowsy', 'Awake',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Awake',\n       'Drowsy', 'Drowsy', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Drowsy',\n       'Drowsy', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake',\n       'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Drowsy', 'Drowsy', 'Awake', 'Drowsy',\n       'Drowsy', 'Drowsy', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Drowsy',\n       'Awake', 'Awake', 'Awake', 'Awake', 'Drowsy', 'Awake', 'Drowsy',\n       'Drowsy', 'Drowsy', 'Awake', 'Awake', 'Drowsy', 'Awake'],\n      dtype=object)"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43my_test\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('body_language.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Make Detections with Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('body_language.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestclassifier', RandomForestClassifier())])",
      "text/html": "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n                (&#x27;randomforestclassifier&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.9 0.1]\n",
      "None\n",
      "Awake [0.82 0.18]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.91 0.09]\n",
      "None\n",
      "Awake [0.91 0.09]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.95 0.05]\n",
      "None\n",
      "Awake [0.93 0.07]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.95 0.05]\n",
      "None\n",
      "Awake [0.95 0.05]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.95 0.05]\n",
      "None\n",
      "Awake [0.95 0.05]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.95 0.05]\n",
      "None\n",
      "Awake [0.95 0.05]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.94 0.06]\n",
      "None\n",
      "Awake [0.95 0.05]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.85 0.15]\n",
      "None\n",
      "Awake [0.83 0.17]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.83 0.17]\n",
      "None\n",
      "Awake [0.8 0.2]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.8 0.2]\n",
      "None\n",
      "Awake [0.79 0.21]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.79 0.21]\n",
      "None\n",
      "Awake [0.78 0.22]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.79 0.21]\n",
      "None\n",
      "Awake [0.78 0.22]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.78 0.22]\n",
      "None\n",
      "Awake [0.78 0.22]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.77 0.23]\n",
      "None\n",
      "Awake [0.77 0.23]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.77 0.23]\n",
      "None\n",
      "Awake [0.77 0.23]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.75 0.25]\n",
      "None\n",
      "Awake [0.77 0.23]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.75 0.25]\n",
      "None\n",
      "Awake [0.75 0.25]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.75 0.25]\n",
      "None\n",
      "Awake [0.75 0.25]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.75 0.25]\n",
      "None\n",
      "Awake [0.75 0.25]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.77 0.23]\n",
      "None\n",
      "Awake [0.75 0.25]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.75 0.25]\n",
      "None\n",
      "Awake [0.61 0.39]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.67 0.33]\n",
      "None\n",
      "Awake [0.8 0.2]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.82 0.18]\n",
      "None\n",
      "Awake [0.81 0.19]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.82 0.18]\n",
      "None\n",
      "Awake [0.82 0.18]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.82 0.18]\n",
      "None\n",
      "Awake [0.82 0.18]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.82 0.18]\n",
      "None\n",
      "Awake [0.81 0.19]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.81 0.19]\n",
      "None\n",
      "Awake [0.82 0.18]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.82 0.18]\n",
      "None\n",
      "Awake [0.81 0.19]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.82 0.18]\n",
      "None\n",
      "Awake [0.81 0.19]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.92 0.08]\n",
      "None\n",
      "Awake [0.99 0.01]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [0.98 0.02]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drowsy [0.38 0.62]\n",
      "Drowsy [0.28 0.72]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [0.9 0.1]\n",
      "None\n",
      "Awake [0.99 0.01]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [1. 0.]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awake [1. 0.]\n",
      "None\n",
      "Awake [0.88 0.12]\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\alex\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drowsy [0.42 0.58]\n",
      "Drowsy [0.33 0.67]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "value = 0\n",
    "class Timer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        text=\"Elapsed time: {:0.4f} seconds\",\n",
    "        logger=print\n",
    "    ):\n",
    "        self._start_time = None\n",
    "        self.text = text\n",
    "        self.logger = logger\n",
    "\n",
    "    # Other methods are unchanged\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer, and report the elapsed time\"\"\"\n",
    "        if self._start_time is None:\n",
    "            return None\n",
    "\n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "\n",
    "        if self.logger:\n",
    "            self.logger(self.text.format(elapsed_time))\n",
    "\n",
    "        return elapsed_time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "prev = None\n",
    "t = Timer(text=\"You waited {:.1f} seconds\")\n",
    "# Initiate holistic model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Recolor Feed\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Make Detections\n",
    "        results = holistic.process(image)\n",
    "        # print(results.face_landmarks)\n",
    "        \n",
    "        # face_landmarks, pose_landmarks, left_hand_landmarks, right_hand_landmarks\n",
    "        \n",
    "        # Recolor image back to BGR for rendering\n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Draw face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        # Export coordinates\n",
    "        try:\n",
    "            # Extract Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extract Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())\n",
    "            \n",
    "            # Concate rows\n",
    "            row = face_row\n",
    "\n",
    "            # Make Detections\n",
    "            X = pd.DataFrame([row])\n",
    "            body_language_class = model.predict(X)[0]\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            print(body_language_class, body_language_prob)\n",
    "\n",
    "            if prev != body_language_class:\n",
    "                prev = body_language_class\n",
    "                if prev == \"Drowsy\":\n",
    "                    from timer import Timer\n",
    "                    t.start()\n",
    "                if prev == \"Awake\":\n",
    "                    value = t.stop()\n",
    "\n",
    "            # Grab ear coords\n",
    "            coords = tuple(np.multiply(\n",
    "                            np.array(\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                        , [640,480]).astype(int))\n",
    "            \n",
    "            cv2.rectangle(image, \n",
    "                          (coords[0], coords[1]+5), \n",
    "                          (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                          (245, 117, 16), -1)\n",
    "            cv2.putText(image, \"Bad\" if body_language_class == \"Drowsy\" else \"Good\", coords,\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            cv2.putText(image, value, coords,\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            print(value)\n",
    "            # Get status box\n",
    "            cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            # Display Class\n",
    "            cv2.putText(image, 'Posture'\n",
    "                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, \"Bad\" if body_language_class == \"Drowsy\" else \"Good\"\n",
    "                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Display Probability\n",
    "            cv2.putText(image, 'PROB'\n",
    "                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tuple(np.multiply(np.array((results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y)), [640,480]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}